<!DOCTYPE HTML>
<html lang="en">
  <head>

    <!-- Google tag (gtag.js) -->
    <script async
      src="https://www.googletagmanager.com/gtag/js?id=G-Z80P1GGMNH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-Z80P1GGMNH');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jinwoo Kim</title>

    <meta name="author" content="Jinwoo Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico"
      type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Jinwoo Kim
                    </p>
                    <p style="text-align:center">
                      jinwoo-kim [at] kaist.ac.kr
                    </p>
                    <p>
                      I am a Ph.D. student at <a href="https://www.kaist.ac.kr/en/">KAIST</a>
                      advised by <a href="https://maga33.github.io/">Seunghoon Hong</a>
                      and a visiting scholar at NYU working with
                      <a href="https://kyunghyuncho.me/">Kyunghyun Cho</a>.
                      My name 진우 眞友 is pronounced [jeen-oo] in Korean.
                    </p>
                    <p>
                      I am interested in making current deep learning models
                      generalize better out of their training data so that they
                      can solve challenging problems such as scientific discovery.
                      I have been studying this problem primarily in the context of
                      geometric deep learning, focusing on making general neural
                      networks including vision and language models behave
                      predictably when they encounter novel, transformed inputs.
                      I often use tools from theories of graphs,
                      (semi)groups and manifolds, and Markov processes
                      such as random walks, diffusions and flows.
                    </p>
                    <p style="text-align:center">
                      <a href="data/jinwookim_cv_20251020.pdf">CV</a>
                      &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?hl=en&user=kSJAiE4AAAAJ">Google Scholar</a>
                      &nbsp;/&nbsp;
                      <a href="https://github.com/jw9730">GitHub</a>
                      &nbsp;/&nbsp;
                      <a href="https://twitter.com/jw9730">X</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/jw9730">LinkedIn</a>
                    </p>
                  </td>
                  <td
                    style="padding:2.5%;width:30%;max-width:40%;vertical-align:middle">
                    <a href="images/jinwookim-profile.jpeg"><img
                        style="width:180px; height:180px; object-fit: cover"
                        alt="profile photo"
                        src="images/jinwookim-profile.jpeg"
                        class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <h2>Experience</h2>
                      <p>
                        <strong>New York University</strong> <br>
                        Visiting Scholar, Nov 2025 &ndash; Jan 2026 (Host: <a
                          href="https://scholar.google.com/citations?user=0RAmmIAAAAAJ&hl=en">Kyunghyun
                          Cho</a>)
                      </p>
                      <p>
                        <strong>LG AI Research</strong> <br>
                        Research Intern, 2022 (Host: <a
                          href="https://scholar.google.com/citations?user=BMvYy9cAAAAJ&hl=en">Moontae
                          Lee</a> and <a
                          href="https://scholar.google.com/citations?user=fmSHtE8AAAAJ&hl=en">Honglak
                          Lee</a>)
                      </p>
                    </td>
                  </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:0px;">
                        <h2>Selected Publications</h2>
                        <p>
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2510.01510" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Flock: A Knowledge Graph Foundation Model via Learning on Random Walks</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim*</u>, Xingyue Huang*, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail İlkan Ceylan
                        <br>
                        Preprint, 2025
                        <br>
                        <a href="https://arxiv.org/abs/2510.01510">paper</a> /
                        <a href="https://github.com/jw9730/flock-pytorch">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2510.15366" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Sequence Modeling with Spectral Mean Flows</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Max Beier, Petar Bevanda, Nayun Kim, Seunghoon Hong
                        <br>
                        NeurIPS, 2025
                        <br>
                        <a href="https://arxiv.org/abs/2510.15366">paper</a> /
                        <a href="https://github.com/jw9730/spectral-mean-flow">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2407.01214" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Revisiting Random Walks for
                            Learning on Graphs</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Olga Zaghen*,
                        Ayhan Suleymanzade*, Youngmin Ryou, Seunghoon
                        Hong
                        <br>
                        ICLR, 2025 <font color="red">Spotlight Presentation (3%)</font>; ICML GRaM Workshop, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2407.01214">paper</a> /
                        <a href="https://github.com/jw9730/random-walk">code</a> /
                        <a href="https://drive.google.com/file/d/16Xqs1afU-o6UqcLyNBs1zfHb3lfsKUSO/view?usp=sharing">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2306.02866" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Learning Probabilistic
                            Symmetrization for Architecture Agnostic
                            Equivariance</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Tien Dat Nguyen, Ayhan
                        Suleymanzade, Hyeokjun An, Seunghoon Hong
                        <br>
                        NeurIPS, 2023 <font color="red">Spotlight Presentation (3%)</font>
                        <br>
                        <a href="https://arxiv.org/abs/2306.02866">paper</a> /
                        <a href="https://github.com/jw9730/lps">code</a> /
                        <a href="https://drive.google.com/file/d/1sKQMzrJp79dYAx9Gqv1a63ltwrF6fRUG/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1BpfzPXZepUKU4aNNCAf0-X5-dyy49wQPNKKkPCXiJoM/edit?usp=drive_link">slides</a>
                        /
                        <a href="https://docs.google.com/presentation/d/15jGNVcWjxP5H-lXHJvpYd6yRw3fRcpbHYB0Ob6DMFIs/edit?usp=drive_link">extended slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2303.14969" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Universal Few-shot Learning
                            of Dense Prediction Tasks with Visual Token Matching</span>
                        </a>
                        <br>
                        Donggyun Kim, <u>Jinwoo Kim</u>, Seongwoong
                        Cho, Chong Luo, Seunghoon Hong
                        <br>
                        ICLR, 2023 <font color="red">Outstanding Paper Award (0.08%)</font>
                        <br>
                        <a href="https://arxiv.org/abs/2303.14969">paper</a> /
                        <a href="https://github.com/GitGyun/visual_token_matching">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2207.02505" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Pure Transformers are
                            Powerful
                            Graph Learners</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Tien Dat Nguyen, Seonwoo
                        Min,
                        Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong
                        <br>
                        NeurIPS, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2207.02505">paper</a> /
                        <a href="https://github.com/jw9730/tokengt">code</a> /
                        <a href="https://www.youtube.com/watch?v=TAKyjYoimd0">talk</a>
                        /
                        <a href="https://drive.google.com/file/d/1RhvZVH7qIc78DJp5MOYsjZnt9wwujR0b/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1v2ieNHQXqAxwDXFRU9U8GndsAeoXRrpmTwrFT1gOVXk/edit?usp=drive_link">slides</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1Evvro8fmjhe3GwaZarpfQz0QYuwsI-JkG2xhYqdNR_Q/edit?usp=drive_link">extended slides</a>
                        <br>
                      </td>
                    </tr>

                  </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:40px;padding-bottom:0px;">
                        <h2>Full Publications</h2>
                        <p>
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <span class="papertitle">Inverting Data Transformations via Diffusion Sampling</span>
                        <br>
                        <u>Jinwoo Kim*</u>, Sékou-Oumar Kaba*, Jiyun Park, Seunghoon Hong, Siamak Ravanbakhsh
                        <br>
                        Under review, 2025
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2510.01510" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Flock: A Knowledge Graph Foundation Model via Learning on Random Walks</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim*</u>, Xingyue Huang*, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail İlkan Ceylan
                        <br>
                        Preprint, 2025
                        <br>
                        <a href="https://arxiv.org/abs/2510.01510">paper</a> /
                        <a href="https://github.com/jw9730/flock-pytorch">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2510.15366" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Sequence Modeling with Spectral Mean Flows</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Max Beier, Petar Bevanda, Nayun Kim, Seunghoon Hong
                        <br>
                        NeurIPS, 2025
                        <br>
                        <a href="https://arxiv.org/abs/2510.15366">paper</a> /
                        <a href="https://github.com/jw9730/spectral-mean-flow">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2407.01214" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Revisiting Random Walks for
                            Learning on Graphs</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Olga Zaghen*,
                        Ayhan Suleymanzade*, Youngmin Ryou, Seunghoon
                        Hong
                        <br>
                        ICLR, 2025 <font color="red">Spotlight
                          Presentation (380/11672=3.26%)</font>; ICML GRaM Workshop, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2407.01214">paper</a> /
                        <a href="https://github.com/jw9730/random-walk">code</a> /
                        <a href="https://drive.google.com/file/d/16Xqs1afU-o6UqcLyNBs1zfHb3lfsKUSO/view?usp=sharing">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2309.04062" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">3D Denoisers are Good 2D
                            Teachers: Molecular Pretraining via Denoising and
                            Cross-Modal Distillation</span>
                        </a>
                        <br>
                        Sungjun Cho, Dae-Woong Jeong, Sung Moon Ko,
                        <u>Jinwoo Kim</u>, Sehui Han, Seunghoon Hong, Honglak Lee,
                        Moontae Lee
                        <br>
                        AAAI, 2025 <font color="red">Oral Presentation</font>
                        <br>
                        <a href="https://arxiv.org/abs/2309.04062">paper</a>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2410.22918" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Simulation-Free Training of
                            Neural ODEs on Paired Data</span>
                        </a>
                        <br>
                        Semin Kim*, Jaehoon Yoo*, <u>Jinwoo Kim</u>,
                        Yeonwoo Cha, Saehoon Kim, Seunghoon Hong
                        <br>
                        NeurIPS, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2410.22918">paper</a> /
                        <a href="https://github.com/seminkim/simulation-free-node">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2311.07143" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Learning Symmetrization for
                            Equivariance with Orbit Distance Minimization</span>
                        </a>
                        <br>
                        Tien Dat Nguyen*, <u>Jinwoo Kim*</u>,
                        Hongseok Yang, Seunghoon Hong
                        <br>
                        NeurIPS NeurReps Workshop, 2023
                        <br>
                        <a href="https://arxiv.org/abs/2311.07143">paper</a> /
                        <a href="https://github.com/tiendatnguyen-vision/Orbit-symmetrize">code</a>
                        /
                        <a href="https://drive.google.com/file/d/1EgGdGV_ZgQSLbuphk3NtK5FPGxAlp8au/view?usp=drive_link">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2306.02866" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Learning Probabilistic
                            Symmetrization for Architecture Agnostic
                            Equivariance</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Tien Dat Nguyen, Ayhan
                        Suleymanzade, Hyeokjun An, Seunghoon Hong
                        <br>
                        NeurIPS, 2023 <font color="red">Spotlight
                            Presentation (378/12345=3.06%)</font>
                        <br>
                        <a href="https://arxiv.org/abs/2306.02866">paper</a> /
                        <a href="https://github.com/jw9730/lps">code</a> /
                        <a href="https://drive.google.com/file/d/1sKQMzrJp79dYAx9Gqv1a63ltwrF6fRUG/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1BpfzPXZepUKU4aNNCAf0-X5-dyy49wQPNKKkPCXiJoM/edit?usp=drive_link">slides</a>
                        /
                        <a href="https://docs.google.com/presentation/d/15jGNVcWjxP5H-lXHJvpYd6yRw3fRcpbHYB0Ob6DMFIs/edit?usp=drive_link">extended slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2303.14969" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Universal Few-shot Learning
                            of
                            Dense Prediction Tasks with Visual Token
                            Matching</span>
                        </a>
                        <br>
                        Donggyun Kim, <u>Jinwoo Kim</u>, Seongwoong
                        Cho, Chong Luo, Seunghoon Hong
                        <br>
                        ICLR, 2023 <font color="red">Outstanding Paper
                            Award (4/4955=0.08%)</font>
                        <br>
                        <a href="https://arxiv.org/abs/2303.14969">paper</a> /
                        <a href="https://github.com/GitGyun/visual_token_matching">code</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2207.02505" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Pure Transformers are
                            Powerful
                            Graph Learners</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Tien Dat Nguyen, Seonwoo
                        Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong
                        <br>
                        NeurIPS, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2207.02505">paper</a> /
                        <a href="https://github.com/jw9730/tokengt">code</a> /
                        <a href="https://www.youtube.com/watch?v=TAKyjYoimd0">talk</a>
                        /
                        <a href="https://drive.google.com/file/d/1RhvZVH7qIc78DJp5MOYsjZnt9wwujR0b/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1v2ieNHQXqAxwDXFRU9U8GndsAeoXRrpmTwrFT1gOVXk/edit?usp=drive_link">slides</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1Evvro8fmjhe3GwaZarpfQz0QYuwsI-JkG2xhYqdNR_Q/edit?usp=drive_link">extended slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2210.15541" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Transformers Meet Stochastic
                            Block Models: Attention with Data-Adaptive Sparsity
                            and Cost</span>
                        </a>
                        <br>
                        Sungjun Cho, Seonwoo Min, <u>Jinwoo Kim</u>,
                        Moontae Lee, Honglak Lee, Seunghoon Hong
                        <br>
                        NeurIPS, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2210.15541">paper</a> /
                        <a href="https://github.com/sc782/SBM-Transformer">code</a>
                        /
                        <a href="https://drive.google.com/file/d/1Q2TiCd1RfpsO3ETrKcOoZs1T9LoxboH0/view?usp=drive_link">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2208.10428" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Equivariant Hypergraph Neural
                            Networks</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Saeyoon Oh, Sungjun Cho,
                        Seunghoon Hong
                        <br>
                        ECCV, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2208.10428">paper</a> /
                        <a href="https://github.com/jw9730/ehnn">code</a> /
                        <a href="https://drive.google.com/file/d/1zle2VZnq_UWGh6dIF7tJJlxOWmbFXkwP/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1ld9uXD5wm4y5akehKtfegswnvlTVy3bM6Vl4Jhqm79E/edit?usp=drive_link">slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2110.14416" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Transformers Generalize
                            DeepSets and Can be Extended to Graphs and
                            Hypergraphs</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim</u>, Saeyoon Oh, Seunghoon Hong
                        <br>
                        NeurIPS, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2110.14416">paper</a> /
                        <a href="https://github.com/jw9730/hot">code</a> /
                        <a href="https://drive.google.com/file/d/1UQuxUoqxbXuv0gy4CZWN9JKHNkba_FCZ/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1wwZspiVFLpYIdOaU_Zo2EqokDqM3tDa7HbewIWBf2J4/edit?usp=drive_link">slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2103.15619" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">SetVAE: Learning Hierarchical
                            Composition for Generative Modeling of
                            Set-Structured
                            Data</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim*</u>, Jaehoon Yoo*, Juho Lee,
                        Seunghoon Hong
                        <br>
                        CVPR, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2103.15619">paper</a> /
                        <a href="https://github.com/jw9730/setvae">code</a> /
                        <a href="https://vllab.kaist.ac.kr/viewer/project/kim2021setvae">project page</a> /
                        <a href="https://drive.google.com/file/d/1kA28QdzS5X5ifn4JnAJ60t4nlZ5scZLb/view?usp=drive_link">poster</a>
                        /
                        <a href="https://docs.google.com/presentation/d/1Hr5IUgcHc2_apnSZXWjIfSjQiJuxvZEUjlbPW7fWlbk/edit?usp=drive_link">slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:20px;">
                        <a href="https://www.jneurosci.org/content/40/34/6584" style="color: inherit; text-decoration: none;">
                          <span class="papertitle">Spontaneous Retinal Waves Can
                            Generate Long-Range Horizontal Connectivity in
                            Visual
                            Cortex</span>
                        </a>
                        <br>
                        <u>Jinwoo Kim*</u>, Min Song*, Jaeson Jang, Se-Bum Paik
                        <br>
                        The Journal of Neuroscience 40(34), 2020
                        <br>
                        <a href="https://www.jneurosci.org/content/40/34/6584">paper</a>
                        <br>
                      </td>
                    </tr>

                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Honors</h2>
                        <p>
                          <strong>Outstanding Researcher Award,</strong>
                          KAIST-Mila Prefrontal AI Research Center, 2024 <br>
                          <strong>Recipient,</strong> ELLIS Mobility Grant,
                          ICML 2024 GRaM Workshop <br>
                          <strong>Outstanding Paper Award,</strong> ICLR 2023
                          (as a coauthor) <br>
                          <strong>Silver Prize,</strong> Samsung Humantech Paper
                          Award, 2023 (as a coauthor) <br>
                          <strong>Recipient,</strong> Qualcomm Innovation
                          Fellowship Korea, 2022 <br>
                          <strong>Excellence Award,</strong> KAIST Undergraduate
                          Research Program, 2022 (as a mentor) <br>
                          <strong>Recipient,</strong> Kwanjeong Education
                          Foundation Scholarship, 2022-2023 <br>
                          <strong>Recipient,</strong> KAIST Engineering
                          Innovator
                          Award, 2020 <strong>(Top 5 in
                              College of Engineering)</strong> <br>
                          <strong>Recipient,</strong> National Science &
                          Technology Scholarship, 2018-2020 <br>
                          <strong>Recipient,</strong> KAIST Alumni Fellowship,
                          2017-2020 <br>
                          <strong>Recipient,</strong> KAIST Presidental
                          Fellowship, 2016-2020 <br>
                          <strong>Recipient,</strong> KAIST Dean's List, Spring
                          2016 / Fall 2016 / Spring 2018 <br>
                          <strong>Recipient,</strong> Hansung Scholarship for
                          Gifted Students, 2015-2016
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Invited Talks</h2>
                        <p>
                          <strong>Extrinsic Symmetries for Neural Networks</strong> <br>
                          Jul 2025, Dec 2024: Mila - Quebec AI Institute <br>
                          May 2025: KAIST AI899 Geometric DL <br>
                          Nov 2024: KAIST-Mila Prefrontal AI Research Center <br>
                          Aug 2024: Sungkyunkwan University (SKKU) <br>
                          Nov 2023: Pohang University of Science and Technology
                          (POSTECH)
                        </p>
                        <p>
                          <strong>Universal Few-shot Learning of Dense
                            Prediction
                            Tasks with Visual Token Matching</strong> <br>
                          Aug 2023: KAIST-Samsung Electronics DS Division
                          Exchange Meetup
                        </p>
                        <p>
                          <strong>Pure Transformers are Powerful Graph
                            Learners</strong> <br>
                          Jan 2023: Microsoft USA <br>
                          Nov 2022: NeurIPS 2022 at KAIST <br>
                          Aug 2022: Learning on Graphs and Geometry Reading
                          Group
                          (LoGaG)
                        </p>
                        <p>
                          <strong>Transformers Generalize DeepSets and Can be
                            Extended to Graphs and Hypergraphs</strong> <br>
                          Jan 2023: Qualcomm Korea <br>
                          Jan 2022: KAIST AI Workshop 21/22 <br>
                          Dec 2021: NeurIPS Social: ML in Korea
                        </p>
                        <p>
                          <strong>SetVAE: Learning Hierarchical Composition for
                            Generative Modeling of Set-Structured Data</strong>
                          <br>
                          Sep 2021: Naver AI Author Meetup <br>
                          Sep 2021: Korean Conference on Computer Vision (KCCV)
                        </p>
                        <p>
                          <strong>Spontaneous Retinal Waves Can Generate
                            Long-Range Horizontal Connectivity in Visual
                            Cortex</strong> <br>
                          Oct 2019: Society for Neuroscience (SfN)
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Academic Services</h2>
                        <p>
                          <strong>Conference Reviewer,</strong> AISTATS 2025&ndash;2026,
                          ICLR 2025&ndash;2026, NeurIPS 2022&ndash;2025, ICML 2023&ndash;2025,
                          IJCNN 2025, LoG 2022&ndash;2025, ICCV 2025 SP4V Workshop,
                          ICML 2024 GRaM Workshop, CVPR 2022, ACCV 2022
                          <br>
                          <strong>Journal Reviewer,</strong> IJCV 2025, TMLR 2024,
                          Neural Networks 2023
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Teaching</h2>
                        <p>
                          <strong>Teaching Assistant,</strong> Computer Vision
                          (CS576), Spring 2022 / 2023 <br>
                          <strong>Teaching Assistant,</strong> Introduction to
                          Deep Learning (CS492I / CS371), Fall 2021 / 2022 /
                          2023
                          <br>
                          <strong>Teaching Assistant,</strong> Samsung Research
                          AI
                          Expert Program, Summer 2021 / 2022 / 2023 <br>
                          <strong>Teaching Assistant,</strong> Undergraduate
                          Research Program (URP), Spring 2022 / 2024
                          <br>
                          <strong>Teaching Assistant,</strong> School of
                          Computing
                          Colloquium (CS966 / CS986), Spring 2021
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-bottom:0px;"
                        colspan="2">
                        <h2>Music</h2>
                        <p>
                          I love listening to and making music! My favorite
                          musicians include <a
                            href="https://www.youtube.com/watch?v=meF_vnE_uAw">Lamp</a>,
                          <a
                            href="https://www.youtube.com/watch?v=2FMP-9bn9N8">Radiohead</a>,
                          and <a
                            href="https://www.youtube.com/watch?v=iXYkEH4OsQw">Ryuichi
                            Sakamoto</a>. Below are some of my original
                          compositions:
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td
                        style="padding:20px;width:50%;vertical-align:middle;padding-top:0px;padding-bottom:0px;">
                        <iframe width="100%" height="200"
                          src="https://www.youtube.com/embed/y4kXYNcqRiE?si=Uobmv8odeeR2pHK9"
                          title="YouTube video player" frameborder="0"
                          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                          allowfullscreen></iframe>
                      </td>
                      <td
                        style="padding:20px;width:50%;vertical-align:middle;padding-top:0px;padding-bottom:0px;">
                        <iframe width="100%" height="200"
                          src="https://www.youtube.com/embed/0NGoa0m3pQA?si=HYLNEHN-hOdEmJ_0"
                          title="YouTube video player" frameborder="0"
                          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                          allowfullscreen></iframe>
                      </td>
                    </tr>
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:0px;"
                        colspan="2">
                        <p>
                          I occasionally post music stuff on my <a
                          href="https://blog.naver.com/jw9730_/">blog</a>.
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:left;font-size:small;">
                          Built from <a href="https://jonbarron.info/"
                            style="font-size:small;">Jon Barron</a>'s academic
                          website
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
              </td>
            </tr>
          </table>
        </body>
      </html>
