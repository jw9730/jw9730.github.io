<!DOCTYPE HTML>
<html lang="en">
  <head>

    <!-- Google tag (gtag.js) -->
    <script async
      src="https://www.googletagmanager.com/gtag/js?id=G-Z80P1GGMNH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-Z80P1GGMNH');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jinwoo Kim</title>

    <meta name="author" content="Jinwoo Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico"
      type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table
      style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Jinwoo Kim
                    </p>
                    <p style="text-align:center">
                      jinwoo-kim [at] kaist.ac.kr
                    </p>
                    <p>
                      I am a Ph.D. student at <a
                        href="https://www.kaist.ac.kr/en/">KAIST</a> School of
                      Computing, advised by <a
                        href="https://maga33.github.io/">Seunghoon Hong</a>. My
                      name ÏßÑÏö∞ ÁúûÂèã is pronounced [jeen-oo] in Korean.
                    </p>
                    <p>
                      I am interested in endowing current deep learning models
                      with proper inductive biases to enhance generalization and
                      enable learning from limited data. I have been studying
                      this problem primarily in the context of <a
                        href="https://geometricdeeplearning.com/blogs/">geometric
                        deep learning</a>,
                      focusing on general-purpose deep neural networks that
                      maintain invariances to group symmetries for solving
                      tasks on graphs and structured data.
                    </p>
                    <p style="text-align:center">
                      <a href="data/jinwookim-cv-20240618.pdf">CV</a>
                      &nbsp;/&nbsp;
                      <a
                        href="https://scholar.google.com/citations?hl=en&user=kSJAiE4AAAAJ">Google
                        Scholar</a>
                      &nbsp;/&nbsp;
                      <a href="https://github.com/jw9730">GitHub</a>
                      &nbsp;/&nbsp;
                      <a href="https://twitter.com/jw9730">Twitter</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/jw9730">LinkedIn</a>
                    </p>
                  </td>
                  <td
                    style="padding:2.5%;width:30%;max-width:40%;vertical-align:middle">
                    <a href="images/jinwookim-profile.jpeg"><img
                        style="width:180px; height:180px; object-fit: cover"
                        alt="profile photo"
                        src="images/jinwookim-profile.jpeg"
                        class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody></table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>News</h2>
                    <p>
                      <strong>Jun 2024:</strong> Random Walk Neural Networks
                      was accepted to ICML 2024 <a
                        href="https://gram-workshop.github.io/">GRaM
                        Workshop</a>.
                      <br>
                      <strong>Nov 2023:</strong> I gave an invited presentation
                      on <a
                        href="https://arxiv.org/abs/2306.02866">Probabilistic
                        Symmetrization</a> at <a
                        href="https://ml.postech.ac.kr/">Machine Learning Lab @
                        POSTECH</a>. <br>
                      <strong>Oct 2023:</strong> <a
                        href="https://arxiv.org/abs/2311.07143">Orbit Distance
                        Minimization</a> was accepted to NeurIPS 2023 <a
                        href="https://www.neurreps.org/">NeurReps Workshop</a>.
                      <br>
                      <strong>Sep 2023:</strong> <a
                        href="https://arxiv.org/abs/2306.02866">Probabilistic
                        Symmetrization</a> was accepted to NeurIPS 2023 as a
                      spotlight presentation. <br>
                    </p>
                    <p>
                      <details>
                        <summary>
                          Older
                        </summary>
                        <p>
                          <strong>May 2023:</strong> <a
                            href="https://arxiv.org/abs/2303.14969">Visual Token
                            Matching</a> was introduced in the <a
                            href="https://xtech.nikkei.com/atcl/nxt/mag/rob/18/00007/00063/">latest
                            issue of Nikkei Robotics</a>. <br>
                          <strong>Mar 2023:</strong> <a
                            href="https://arxiv.org/abs/2303.14969">Visual Token
                            Matching</a> won the <a
                            href="https://blog.iclr.cc/2023/03/21/announcing-the-iclr-2023-outstanding-paper-award-recipients/">ICLR
                            2023 outstanding paper award</a>, becoming the first
                          paper from South Korea that received the best paper
                          award at major machine learning conferences.
                          <strong>Jan 2023:</strong> I gave an invited
                          presentation on <a
                            href="https://arxiv.org/abs/2207.02505">Tokenized
                            Graph Transformer</a> at a reading group of
                          Microsoft USA. <br>
                          <strong>Jan 2023:</strong> <a
                            href="https://arxiv.org/abs/2207.02505">Tokenized
                            Graph Transformer</a> was introduced in a <a
                            href="https://huggingface.co/blog/intro-graphml">Hugging
                            Face ü§ó blog article</a> on graph machine learning.
                          <br>
                          <strong>Jan 2023:</strong> <a
                            href="https://arxiv.org/abs/2207.02505">Tokenized
                            Graph Transformer</a> was highlighted as one of the
                          outstanding works on graph transformers in <a
                            href="https://mgalkin.medium.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232">Michael
                            Galkin‚Äôs review article</a> on 2022‚Äôs graph machine
                          learning. <br>
                          <strong>Jan 2023:</strong> <a
                            href="https://arxiv.org/abs/2303.14969">Visual Token
                            Matching</a> was accepted to ICLR 2023 as an oral
                          presentation (notable-top-5%) after being <a
                            href="https://guoqiangwei.xyz/iclr2023_stats/iclr2023_submissions.html">ranked
                            #1 in review ratings among the 4,966
                            submissions</a>. <br>
                          <strong>Jan 2023:</strong> I gave an invited
                          presentation on <a
                            href="https://arxiv.org/abs/2110.14416">Higher-order
                            Transformers</a> at Qualcomm Research Korea. <br>
                          <strong>Aug 2022:</strong> I gave an invited
                          presentation on <a
                            href="https://arxiv.org/abs/2207.02505">Tokenized
                            Graph Transformer</a> at <a
                            href="https://hannes-stark.com/logag-reading-group">Learning
                            on Graphs and Geometry Reading Group (LoGaG)</a>.
                          <br>
                        </p>
                      </details>
                    </p>
                  </td>
                </tr>
              </tbody></table>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-bottom:0px;">
                        <h2>Research</h2>
                        <p>
                          * denotes equal contribution. Representative papers
                          are <span class="highlight">highlighted</span>.
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <tr>
                      <td bgcolor="#ffffd0"
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <span class="papertitle">Revisiting Random Walks for
                          Learning on Graphs</span>
                        <br>
                        <strong>Jinwoo Kim</strong>, Olga Zaghen*,
                        Ayhan Suleymanzade*, Youngmin Ryou, Seunghoon
                        Hong
                        <br>
                        <em>ICML Workshop on Geometry-grounded
                          Representation Learning and Generative
                          Modeling</em>, 2024
                        <br>
                        <font color="gray">Using a language model to solve
                          problems on graphs by processing text record
                          of random walks.</font>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2311.07143" id="ODM">
                          <span class="papertitle">Learning Symmetrization for
                            Equivariance with Orbit Distance Minimization</span>
                        </a>
                        <br>
                        Tien Dat Nguyen*, <strong>Jinwoo Kim*</strong>,
                        Hongseok Yang, Seunghoon Hong
                        <br>
                        <em>NeurIPS Workshop on Symmetry and Geometry in Neural
                          Representations</em>, 2023
                        <br>
                        <a href="https://arxiv.org/abs/2311.07143">paper</a> /
                        <a
                          href="https://github.com/tiendatnguyen-vision/Orbit-symmetrize">code</a>
                        /
                        <a
                          href="https://drive.google.com/file/d/1EgGdGV_ZgQSLbuphk3NtK5FPGxAlp8au/view?usp=drive_link">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td bgcolor="#ffffd0"
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2306.02866" id="LPS">
                          <span class="papertitle">Learning Probabilistic
                            Symmetrization for Architecture Agnostic
                            Equivariance</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim</strong>, Tien Dat Nguyen, Ayhan
                        Suleymanzade, Hyeokjun An, Seunghoon Hong
                        <br>
                        <em>NeurIPS</em>, 2023 &nbsp<font
                          color="red"><strong>(Spotlight
                            Presentation)</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/2306.02866">paper</a> /
                        <a href="https://github.com/jw9730/lps">code</a> /
                        <a
                          href="https://drive.google.com/file/d/1sKQMzrJp79dYAx9Gqv1a63ltwrF6fRUG/view?usp=drive_link">poster</a>
                        /
                        <a
                          href="https://docs.google.com/presentation/d/1BpfzPXZepUKU4aNNCAf0-X5-dyy49wQPNKKkPCXiJoM/edit?usp=drive_link">slides</a>
                        (<a
                          href="https://docs.google.com/presentation/d/15jGNVcWjxP5H-lXHJvpYd6yRw3fRcpbHYB0Ob6DMFIs/edit?usp=drive_link">extended</a>)
                        <br>
                        <font color="gray">A method for endowing equivariance to
                          arbitrary architectures that can repurpose a ViT to
                          process graphs.</font>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2309.04062" id="MPDD">
                          <span class="papertitle">3D Denoisers are Good 2D
                            Teachers: Molecular Pretraining via Denoising and
                            Cross-Modal Distillation</span>
                        </a>
                        <br>
                        Sungjun Cho, Dae-Woong Jeong, Sung Moon Ko,
                        <strong>Jinwoo
                          Kim</strong>, Sehui Han, Seunghoon Hong, Honglak Lee,
                        Moontae Lee
                        <br>
                        <em>arXiv</em>, 2023
                        <br>
                        <a href="https://arxiv.org/abs/2309.04062">preprint</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td bgcolor="#ffffd0"
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2303.14969" id="VTM">
                          <span class="papertitle">Universal Few-shot Learning
                            of
                            Dense Prediction Tasks with Visual Token
                            Matching</span>
                        </a>
                        <br>
                        Donggyun Kim, <strong>Jinwoo Kim</strong>, Seongwoong
                        Cho,
                        Chong Luo, Seunghoon Hong
                        <br>
                        <em>ICLR</em>, 2023 &nbsp<font
                          color="red"><strong>(Outstanding Paper
                            Award)</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/2303.14969">paper</a> /
                        <a
                          href="https://github.com/GitGyun/visual_token_matching">code</a>
                        <br>
                        <font color="gray">A few-shot learner for any dense
                          vision tasks that combines ViT and patch-level
                          non-parametric matching.</font>
                      </td>
                    </tr>

                    <tr>
                      <td bgcolor="#ffffd0"
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2207.02505" id="TokenGT">
                          <span class="papertitle">Pure Transformers are
                            Powerful
                            Graph Learners</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim</strong>, Tien Dat Nguyen, Seonwoo
                        Min,
                        Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong
                        <br>
                        <em>NeurIPS</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2207.02505">paper</a> /
                        <a href="https://github.com/jw9730/tokengt">code</a> /
                        <a
                          href="https://www.youtube.com/watch?v=TAKyjYoimd0">talk</a>
                        /
                        <a
                          href="https://drive.google.com/file/d/1RhvZVH7qIc78DJp5MOYsjZnt9wwujR0b/view?usp=drive_link">poster</a>
                        /
                        <a
                          href="https://docs.google.com/presentation/d/1v2ieNHQXqAxwDXFRU9U8GndsAeoXRrpmTwrFT1gOVXk/edit?usp=drive_link">slides</a>
                        (<a
                          href="https://docs.google.com/presentation/d/1Evvro8fmjhe3GwaZarpfQz0QYuwsI-JkG2xhYqdNR_Q/edit?usp=drive_link">extended</a>)
                        <br>
                        <font color="gray">A pure transformer devoid of
                          graph-tailored modifications can be powerful for graph
                          learning in theory and practice.</font>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2210.15541" id="SBMTr">
                          <span class="papertitle">Transformers Meet Stochastic
                            Block Models: Attention with Data-Adaptive Sparsity
                            and Cost</span>
                        </a>
                        <br>
                        Sungjun Cho, Seonwoo Min, <strong>Jinwoo Kim</strong>,
                        Moontae Lee, Honglak Lee, Seunghoon Hong
                        <br>
                        <em>NeurIPS</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2210.15541">paper</a> /
                        <a
                          href="https://github.com/sc782/SBM-Transformer">code</a>
                        /
                        <a
                          href="https://drive.google.com/file/d/1Q2TiCd1RfpsO3ETrKcOoZs1T9LoxboH0/view?usp=drive_link">poster</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2208.10428" id="EHNN">
                          <span class="papertitle">Equivariant Hypergraph Neural
                            Networks</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim</strong>, Saeyoon Oh, Sungjun Cho,
                        Seunghoon Hong
                        <br>
                        <em>ECCV</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2208.10428">paper</a> /
                        <a href="https://github.com/jw9730/ehnn">code</a> /
                        <a
                          href="https://drive.google.com/file/d/1zle2VZnq_UWGh6dIF7tJJlxOWmbFXkwP/view?usp=drive_link">poster</a>
                        /
                        <a
                          href="https://docs.google.com/presentation/d/1ld9uXD5wm4y5akehKtfegswnvlTVy3bM6Vl4Jhqm79E/edit?usp=drive_link">slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr bgcolor="#ffffd0">
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2110.14416" id="HoT">
                          <span class="papertitle">Transformers Generalize
                            DeepSets and Can be Extended to Graphs and
                            Hypergraphs</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim</strong>, Saeyoon Oh, Seunghoon Hong
                        <br>
                        <em>NeurIPS</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2110.14416">paper</a> /
                        <a href="https://github.com/jw9730/hot">code</a> /
                        <a
                          href="https://drive.google.com/file/d/1UQuxUoqxbXuv0gy4CZWN9JKHNkba_FCZ/view?usp=drive_link">poster</a>
                        /
                        <a
                          href="https://docs.google.com/presentation/d/1wwZspiVFLpYIdOaU_Zo2EqokDqM3tDa7HbewIWBf2J4/edit?usp=drive_link">slides</a>
                        <br>
                        <font color="gray">A generalization of transformers to
                          sets and (hyper)graphs based on higher-order
                          permutation
                          equivariance.</font>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2103.15619" id="SetVAE">
                          <span class="papertitle">SetVAE: Learning Hierarchical
                            Composition for Generative Modeling of
                            Set-Structured
                            Data</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim*</strong>, Jaehoon Yoo*, Juho Lee,
                        Seunghoon Hong
                        <br>
                        <em>CVPR</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2103.15619">paper</a> /
                        <a href="https://github.com/jw9730/setvae">code</a> /
                        <a
                          href="https://vllab.kaist.ac.kr/viewer/project/kim2021setvae">project
                          page</a> /
                        <a
                          href="https://drive.google.com/file/d/1kA28QdzS5X5ifn4JnAJ60t4nlZ5scZLb/view?usp=drive_link">poster</a>
                        /
                        <a
                          href="https://docs.google.com/presentation/d/1Hr5IUgcHc2_apnSZXWjIfSjQiJuxvZEUjlbPW7fWlbk/edit?usp=drive_link">slides</a>
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px">
                        <a href="https://www.jneurosci.org/content/40/34/6584"
                          id="RWave">
                          <span class="papertitle">Spontaneous Retinal Waves Can
                            Generate Long-Range Horizontal Connectivity in
                            Visual
                            Cortex</span>
                        </a>
                        <br>
                        <strong>Jinwoo Kim*</strong>, Min Song*, Jaeson Jang,
                        Se-Bum Paik
                        <br>
                        <em>The Journal of Neuroscience 40(34)</em>, 2020
                        <br>
                        <a
                          href="https://www.jneurosci.org/content/40/34/6584">paper</a>
                        /
                        <a href="hhttps://github.com/vsnnlab/rwave">code</a>
                        <br>
                      </td>
                    </tr>

                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Experience</h2>
                        <p>
                          <strong>LG AI Research Fundamental Research Lab
                            (FRL)</strong> <br>
                          Research Intern, 2022 (Mentors: <a
                            href="https://scholar.google.com/citations?user=BMvYy9cAAAAJ&hl=en">Moontae
                            Lee</a> and <a
                            href="https://scholar.google.com/citations?user=fmSHtE8AAAAJ&hl=en">Honglak
                            Lee</a>)
                        </p>
                        <p>
                          <strong>Korea Advanced Institute of Science and
                            Technology (KAIST)</strong> <br>
                          Research Intern, 2020 (Mentors: <a
                            href="https://scholar.google.com/citations?user=hvr3ALkAAAAJ&hl=en">Seunghoon
                            Hong</a> and <a
                            href="https://scholar.google.co.kr/citations?user=Py4URJUAAAAJ&hl=en">Juho
                            Lee</a>)
                        </p>
                        <p>
                          <strong>Korea Advanced Institute of Science and
                            Technology (KAIST)</strong> <br>
                          Research Intern, 2018-2019 (Mentor: <a
                            href="https://scholar.google.com/citations?user=VQK2PP0AAAAJ&hl=en">Se-Bum
                            Paik</a>)
                        </p>
                        <p>
                          <strong>Korea Advanced Institute of Science and
                            Technology (KAIST)</strong> <br>
                          Research Intern, 2017 (Mentor: <a
                            href="https://scholar.google.com/citations?user=vDkAFQIAAAAJ&hl=en">Doyun
                            Lee</a>)
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Honors</h2>
                        <p>
                          <strong>Outstanding Paper Award,</strong> ICLR 2023
                          (as
                          a coauthor) <br>
                          <strong>Silver Prize,</strong> Samsung Humantech Paper
                          Award, 2023 (as a coauthor) <br>
                          <strong>Recipient,</strong> Qualcomm Innovation
                          Fellowship Korea, 2022 <br>
                          <strong>Recipient,</strong> KAIST Undergraduate
                          Research
                          Program Excellence Award, 2022 (as a mentor) <br>
                          <strong>Recipient,</strong> Kwanjeong Education
                          Foundation Scholarship, 2022-2023 <br>
                          <strong>Recipient,</strong> KAIST Engineering
                          Innovator
                          Award, 2020 &nbsp<font color="red"><strong>(Top 5 in
                              College of Engineering)</strong></font> <br>
                          <strong>Recipient,</strong> National Science &
                          Technology Scholarship, 2018-2020 <br>
                          <strong>Recipient,</strong> KAIST Alumni Fellowship,
                          2017-2020 <br>
                          <strong>Recipient,</strong> KAIST Presidental
                          Fellowship, 2016-2020 <br>
                          <strong>Recipient,</strong> KAIST Dean's List, Spring
                          2016 / Fall 2016 / Spring 2018 <br>
                          <strong>Recipient,</strong> Hansung Scholarship for
                          Gifted Students, 2015-2016
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Invited Talks</h2>
                        <p>
                          <strong>Learning Probabilistic Symmetrization for
                            Architecture Agnostic Equivariance</strong> <br>
                          Nov 2023: Pohang University of Science and Technology
                          (POSTECH)
                        </p>
                        <p>
                          <strong>Universal Few-shot Learning of Dense
                            Prediction
                            Tasks with Visual Token Matching</strong> <br>
                          Aug 2023: KAIST-Samsung Electronics DS Division
                          Exchange
                          Meetup
                        </p>
                        <p>
                          <strong>Pure Transformers are Powerful Graph
                            Learners</strong> <br>
                          Jan 2023: Microsoft USA <br>
                          Nov 2022: NeurIPS 2022 at KAIST <br>
                          Aug 2022: Learning on Graphs and Geometry Reading
                          Group
                          (LoGaG)
                        </p>
                        <p>
                          <strong>Transformers Generalize DeepSets and Can be
                            Extended to Graphs and Hypergraphs</strong> <br>
                          Jan 2023: Qualcomm Korea <br>
                          Jan 2022: KAIST AI Workshop 21/22 <br>
                          Dec 2021: NeurIPS Social: ML in Korea
                        </p>
                        <p>
                          <strong>SetVAE: Learning Hierarchical Composition for
                            Generative Modeling of Set-Structured Data</strong>
                          <br>
                          Sep 2021: Naver AI Author Meetup <br>
                          Sep 2021: Korean Conference on Computer Vision (KCCV)
                        </p>
                        <p>
                          <strong>Spontaneous Retinal Waves Can Generate
                            Long-Range Horizontal Connectivity in Visual
                            Cortex</strong> <br>
                          Oct 2019: Society for Neuroscience (SfN)
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Academic Services</h2>
                        <p>
                          <strong>Conference Reviewer,</strong> NeurIPS 2022 -
                          2024, ICML 2023, CVPR 2022, LoG 2022 / 2023, ACCV 2022
                          <br>
                          <strong>Journal Reviewer,</strong> Neural Networks
                          2023
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Teaching</h2>
                        <p>
                          <strong>Teaching Assistant,</strong> Computer Vision
                          (CS576), Spring 2022 / 2023 <br>
                          <strong>Teaching Assistant,</strong> Introduction to
                          Deep Learning (CS492I / CS371), Fall 2021 / 2022 /
                          2023
                          <br>
                          <strong>Teaching Assistant,</strong> Samsung Research
                          AI
                          Expert Program, Summer 2021 / 2022 / 2023 <br>
                          <strong>Teaching Assistant,</strong> Undergraduate
                          Research Program (URP), Spring 2022 &nbsp<font
                            color="red"><strong>(Excellence
                              Award)</strong></font>
                          <br>
                          <strong>Teaching Assistant,</strong> School of
                          Computing
                          Colloquium (CS966 / CS986), Spring 2021
                        </p>
                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-bottom:0px;"
                        colspan="2">
                        <h2>Music</h2>
                        <p>
                          I love listening to and making music! My favorite
                          musicians include <a
                            href="https://www.youtube.com/watch?v=meF_vnE_uAw">Lamp</a>,
                          <a
                            href="https://www.youtube.com/watch?v=2FMP-9bn9N8">Radiohead</a>,
                          and <a
                            href="https://www.youtube.com/watch?v=iXYkEH4OsQw">Ryuichi
                            Sakamoto</a>. Below are some of my original
                          compositions:
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td
                        style="padding:20px;width:50%;vertical-align:middle;padding-top:0px;">
                        <iframe width="100%" height="200"
                          src="https://www.youtube.com/embed/y4kXYNcqRiE?si=Uobmv8odeeR2pHK9"
                          title="YouTube video player" frameborder="0"
                          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                          allowfullscreen></iframe>
                      </td>
                      <td
                        style="padding:20px;width:50%;vertical-align:middle;padding-top:0px;">
                        <iframe width="100%" height="200"
                          src="https://www.youtube.com/embed/0NGoa0m3pQA?si=HYLNEHN-hOdEmJ_0"
                          title="YouTube video player" frameborder="0"
                          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                          allowfullscreen></iframe>
                      </td>
                    </tr>
                  </tbody></table>

                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:left;font-size:small;">
                          Last updated: May 2024
                        </p>
                      </td>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                          Built from <a href="https://jonbarron.info/"
                            style="font-size:small;">Jon Barron</a>'s academic
                          website
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
              </td>
            </tr>
          </table>
        </body>
      </html>
