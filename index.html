<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="format-detection" content="telephone=no">
  <title>Jinwoo Kim</title>
  <meta name="author" content="Jinwoo Kim">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <style>
    body {
      max-width: 620px;
      margin: 20px auto;
      padding: 0 20px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z80P1GGMNH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-Z80P1GGMNH');
  </script>
</head>

<body>

  <h1>Jinwoo Kim</h1>

  <p><img src="images/jinwookim-profile.jpeg" alt="Jinwoo Kim" width="180" height="180"></p>

  <p>jinwoo-kim [at] kaist.ac.kr</p>

  <p>
    <a href="data/jinwookim_cv_20260206.pdf">CV</a> /
    <a href="https://scholar.google.com/citations?hl=en&user=kSJAiE4AAAAJ">Scholar</a> /
    <a href="https://github.com/jw9730">GitHub</a> /
    <a href="https://twitter.com/jw9730">X</a> /
    <a href="https://www.linkedin.com/in/jw9730">LinkedIn</a>
  </p>

  <p>
    I am a technical research personnel at <a href="https://www.kaist.ac.kr/en/">KAIST</a>.
    I was previously a visiting scholar at <a
      href="https://www.linkedin.com/company/global-ai-frontier-lab/posts/?feedView=all">NYU Global AI Frontier Lab</a>
    working with <a href="https://kyunghyuncho.me/">Kyunghyun Cho</a> and <a
      href="https://cims.nyu.edu/~rajeshr/">Rajesh Ranganath</a>.
    I received my Ph.D. from KAIST advised by <a href="https://maga33.github.io/">Seunghoon Hong</a>.
    My name 진우 眞友 is pronounced [jeen-oo].
  </p>

  <p>
    I am interested in making deep learning models generalize better out of their training data so that they can be used
    to solve challenging problems, such as those in scientific domains. I have been studying this problem primarily from
    the viewpoint of invariances, focusing on all-purpose deep neural networks that can reliably reason upon novel
    transformed inputs. I often use tools from theories of graphs, manifolds, (semi)groups and categories, and Markov
    processes such as random walks, diffusions and flows.<br><br>
    <b>I will be entering the job market in Fall 2026 for research positions starting in early 2027.</b>
  </p>

  <h2>Experience</h2>

  <p>
    <b>KAIST</b> - Expert Research Personnel (<a
      href="https://en.wikipedia.org/wiki/Conscription_in_South_Korea">what?</a>), 03/2026 &ndash; 02/2027<br>
    <b>New York University</b> - Visiting Scholar, 11/2025 &ndash; 01/2026<br>
    <b>LG AI Research</b> - Research Intern, 2022
  </p>

  <h2>Selected Papers</h2>

  <p>
    One-step Language Modeling via Continuous Denoising<br>
    Chanhyuk Lee, Jaehoon Yoo, Manan Agarwal, Sheel Shah, Jerry Huang, Aditi Raghunathan, Seunghoon Hong,
    Nicholas M. Boffi<sup>&dagger;</sup>, <u>Jinwoo Kim</u><sup>&dagger;</sup><br>
    arXiv 2026<br>
    [<a href="https://arxiv.org/abs/2602.16813">paper</a>] [<a href="https://github.com/david3684/flm">code</a>]
    [<a href="https://one-step-lm.github.io/">project page</a>]
  </p>

  <p>
    Architecture-Agnostic Invariances for Deep Learning<br>
    <u>Jinwoo Kim</u><br>
    Ph.D. Dissertation, 2026. <b>KAIST CoE Best Dissertation Award</b><br>
    [<a href="https://drive.google.com/file/d/1kowZhLcFB7w4poHj2xb48UZ-D_VdyjQC/view?usp=drive_link">pdf</a>]
  </p>

  <p>
    Inverting Data Transformations via Diffusion Sampling<br>
    <u>Jinwoo Kim</u>*, Sékou-Oumar Kaba*, Jiyun Park, Seunghoon Hong<sup>&dagger;</sup>, Siamak
    Ravanbakhsh<sup>&dagger;</sup><br>
    arXiv 2026<br>
    [<a href="https://arxiv.org/abs/2602.08267">paper</a>] [<a href="https://github.com/jw9730/tied">code</a>]
  </p>

  <p>
    Flock: A Knowledge Graph Foundation Model via Learning on Random Walks<br>
    <u>Jinwoo Kim</u>*, Xingyue Huang*, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail
    İlkan
    Ceylan<br>
    ICLR, 2026<br>
    [<a href="https://arxiv.org/abs/2510.01510">paper</a>] [<a href="https://github.com/jw9730/flock-pytorch">code</a>]
  </p>

  <p>
    Sequence Modeling with Spectral Mean Flows<br>
    <u>Jinwoo Kim</u>, Max Beier, Petar Bevanda, Nayun Kim, Seunghoon Hong<br>
    NeurIPS, 2025<br>
    [<a href="https://arxiv.org/abs/2510.15366">paper</a>] [<a
      href="https://github.com/jw9730/spectral-mean-flow">code</a>]
  </p>

  <p>
    Revisiting Random Walks for Learning on Graphs<br>
    <u>Jinwoo Kim</u>, Olga Zaghen*, Ayhan Suleymanzade*, Youngmin Ryou, Seunghoon Hong<br>
    ICLR, 2025. <b>Spotlight (3%)</b><br>
    [<a href="https://arxiv.org/abs/2407.01214">paper</a>] [<a href="https://github.com/jw9730/random-walk">code</a>]
  </p>

  <p>
    Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance<br>
    <u>Jinwoo Kim</u>, Tien Dat Nguyen, Ayhan Suleymanzade, Hyeokjun An, Seunghoon Hong<br>
    NeurIPS, 2023. <b>Spotlight (3%)</b><br>
    [<a href="https://arxiv.org/abs/2306.02866">paper</a>] [<a href="https://github.com/jw9730/lps">code</a>]
  </p>

  <p>
    Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching<br>
    Donggyun Kim, <u>Jinwoo Kim</u>, Seongwoong Cho, Chong Luo, Seunghoon Hong<br>
    ICLR, 2023. <b>Outstanding Paper Award (0.08%)</b><br>
    [<a href="https://arxiv.org/abs/2303.14969">paper</a>] [<a
      href="https://github.com/GitGyun/visual_token_matching">code</a>]
  </p>

  <p>
    Pure Transformers are Powerful Graph Learners<br>
    <u>Jinwoo Kim</u>, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee<sup>&dagger;</sup>, Seunghoon
    Hong<sup>&dagger;</sup><br>
    NeurIPS, 2022<br>
    [<a href="https://arxiv.org/abs/2207.02505">paper</a>] [<a href="https://github.com/jw9730/tokengt">code</a>]
  </p>

  <p>
    <a href="publications.html">Full list of publications</a>
  </p>

  <h2>Honors</h2>

  <p>
    Best Ph.D. Dissertation Award, KAIST College of Engineering, 2026<br>
    Outstanding Researcher Award, KAIST-Mila Prefrontal AI Research Center, 2024<br>
    ELLIS Mobility Grant, ICML 2024 GRaM Workshop<br>
    Outstanding Paper Award, ICLR 2023 (as coauthor)<br>
    Silver Prize, Samsung Humantech Paper Award, 2023 (as coauthor)<br>
    Qualcomm Innovation Fellowship Korea, 2022<br>
    Excellence Award, KAIST Undergraduate Research Program, 2022 (as mentor)<br>
    Kwanjeong Education Foundation Scholarship, 2022&ndash;2023<br>
    KAIST Engineering Innovator Award, 2020<br>
    National Science & Technology Scholarship, 2018&ndash;2020<br>
    KAIST Alumni Fellowship, 2017&ndash;2020<br>
    KAIST Presidential Fellowship, 2016&ndash;2020<br>
    KAIST Dean's List, Spring 2016 / Fall 2016 / Spring 2018
  </p>

  <h2>Invited Talks</h2>

  <p>
    Sequence Modeling with Spectral Mean Flows @ BGU, 12/2025<br>
    Architecture-Agnostic Invariances for Deep Learning @ Mila (07/2025, 12/2024), KAIST AI899 (05/2025), KAIST-Mila
    Prefrontal AI Research Center (11/2024), SKKU (08/2024), POSTECH (11/2023)<br>
    Visual Token Matching @ KAIST-Samsung Electronics DS
    Division Exchange Meetup, 08/2023<br>
    Pure Transformers are Powerful Graph Learners @ Microsoft USA (01/2023), NeurIPS 2022 at KAIST (11/2022), LoGaG
    (08/2022)<br>
    Higher-order Transformers for Sets, Graphs, and Hypergraphs @ Qualcomm Korea (01/2023), KAIST AI
    Workshop (01/2022), NeurIPS Social: ML in Korea (12/2021)<br>
    SetVAE @ Naver AI (09/2021), KCCV (09/2021)<br>
    Retinal Waves and Prenatal Wiring of Primary Visual Cortex @ SfN, 10/2019
  </p>

  <h2>Academic Services</h2>

  <p>
    Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, AISTATS, LoG, TAG-DS, IJCNN, ACCV<br>
    Journal Reviewer: TMLR, Neural Networks, IJCV
  </p>

  <h2>Teaching</h2>

  <p>
    TA, Computer Vision (CS576), Spring 2022/2023<br>
    TA, Introduction to Deep Learning (CS492I/CS371), Fall 2021/2022/2023<br>
    TA, Samsung Research AI Expert Program, Summer 2021/2022/2023<br>
    TA, Undergraduate Research Program (URP), Spring 2022/2024
  </p>

  <hr>

  <p>
    I also enjoy <a href="https://blog.naver.com/jw9730_/">making music</a>. Some of my compositions: <a
      href="https://www.youtube.com/watch?v=y4kXYNcqRiE">song 1</a>
    <a href="https://www.youtube.com/watch?v=0NGoa0m3pQA">song 2</a>
  </p>

  <hr>

  <p>
    Last updated: February 2026
  </p>

</body>

</html>